{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isabel2012/Prueba/blob/main/%22_Translated_%5BESP%5D_news_parsing_example_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSO0QRULn21s"
      },
      "source": [
        "### Instalación e importación de las librerías necesarias\n",
        "\n",
        "En este notebook utilizaremos 3 librerías.\n",
        "\n",
        "Veamos para que sirve cada una de ellas:\n",
        "\n",
        "**BeautifulSoup:** Esta biblioteca nos permite analizar código HTML y XML. Proporciona herramientas para extraer datos de páginas web mediante el análisis y recorrido del árbol de elementos HTML/XML.\n",
        "\n",
        "**requests:** Esta librería nos permite enviar solicitudes HTTP. Facilita el envío de solicitudes GET, POST y otros tipos de solicitudes a servidores web. También simplifica el trabajo con servicios web y APIs.\n",
        "\n",
        "**pandas:** Esta biblioteca que permite trabajar con grandes cantidades de datos. Proporciona herramientas convenientes para analizar, manipular y procesar datos estructurados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fF07a3ANfDS"
      },
      "outputs": [],
      "source": [
        "!pip3 install bs4 # En caso de que esta librería no estuviese instalada, se debe ejecutar esta línea para hacerlo.\n",
        "# Google Colaboratory ya posee incorparada esta librería."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjKaKB4zNYqb"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnS80RH8xLBE"
      },
      "source": [
        "### Obtención de los datos desde un sitio web"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQSnRV8ZNZr_"
      },
      "outputs": [],
      "source": [
        "# Enlace del sitio web del cual se quieren obtener los datos\n",
        "url = \"https://www.xataka.com/tag/ciencia-y-tecnologia\" # Ingresar el sitio web como string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeKzlgk1Nt2b"
      },
      "outputs": [],
      "source": [
        "# Utilizamos la biblioteca \"requests\" para enviar una solicitud GET\n",
        "# a la URL especificada y obtener una respuesta.\n",
        "# El estado “200” significa que la solicitud fue procesada correctamente y la página está disponible.\n",
        "\n",
        "response = requests.get(url)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqDnLhJENvXu"
      },
      "outputs": [],
      "source": [
        "# Pasamos el texto del objeto “response” (respuesta) correspondiente al sitio (response.text)\n",
        "# y especificamos el analizador que se utilizará (\"lxml\")\n",
        "# para crear un objeto BeautifulSoup para el análisis posterior del código HTML.\n",
        "\n",
        "bs = BeautifulSoup(response.text,\"lxml\")\n",
        "print(bs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSTgOD7gNxWs"
      },
      "outputs": [],
      "source": [
        "# Aquí, buscamos todos los elementos <h2 class=\"abstract-title\" ...> en la página\n",
        "# y los almacenamos en la variable \"temp\". Estos se corresponden a todos los\n",
        "# enlaces y los títulos de las noticias que queremos recopilar.\n",
        "\n",
        "temp = bs.find_all(\"h2\", \"abstract-title\")\n",
        "print(temp)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Observar qué nos devuelve al imprimir uno de los elementos de \"temp\"\n",
        "\n",
        "print(temp[0]) # Probar con otros valores de index"
      ],
      "metadata": {
        "id": "MTwt9MrsU4q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A modo de ejemplo, se muestra cómo acceder a cada uno de los títulos y enlaces de las noticias\n",
        "# recopiladas en el sitio web. Tener en cuenta que \"temp\" guarda distintos elementos\n",
        "# a los cuales se accede por indexación. Por otro lado, cada uno de esos elementos tiene\n",
        "# diferentes niveles según su organizacion jerarquica. En este caso se accede a los elementos\n",
        "# del tag <a></a>\n",
        "\n",
        "print(\"Acceso al título de la noticia:\", temp[0].a.text) # Prueba con otros valores de index\n",
        "print(\"Acceso al enlace de la noticia:\", temp[0].a.get(\"href\")) # Prueba con otros valores de index"
      ],
      "metadata": {
        "id": "Pd2YYkoHQtue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JYAZCPaOBNN"
      },
      "outputs": [],
      "source": [
        "# Se crea un diccionario vacío con dos elementos cuyas palabras claves son:\n",
        "# \"titulos_noticias\" y \"enlaces\". Aquí se almacenarán los títulos de las noticias\n",
        "# y sus correpsondientes enlaces.\n",
        "\n",
        "dict_news = {\"titulos_noticias\": [], \"enlaces\": []}\n",
        "\n",
        "for i in temp:\n",
        "  dict_news[\"enlaces\"].append(i.a.get(\"href\"))\n",
        "  dict_news[\"titulos_noticias\"].append(i.a.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GFEFot9OFE2"
      },
      "outputs": [],
      "source": [
        "# Utilizamos la biblioteca pandas para crear un DataFrame a partir del\n",
        "# diccionario \"dict_news\". Es importante especificar las columnas \"titulos_noticias\"\n",
        "# y \"enlaces\". Este DataFrame almacenará todos los datos recopilados de los\n",
        "# artículos de noticias.\n",
        "\n",
        "df_news = pd.DataFrame(dict_news, columns=[\"titulos_noticias\", \"enlaces\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC8NRo9QHn5x"
      },
      "outputs": [],
      "source": [
        "# Visualización de los resultados.\n",
        "# Puedes almacenar la información recopilada en cualquier otra estructura si lo deseas.\n",
        "\n",
        "df_news"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}